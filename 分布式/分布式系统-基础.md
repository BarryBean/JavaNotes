# 1. 基础
分布式系统的目的是提升系统的整体性能和吞吐量，并尽量保证容错性。

## 1.1 设计思路
### 1.1.1 中心化
- 两个角色。将集群的节点分为**领导者**和**工作者**。
- 角色职责。领导者负责分发任务，监督工作者，工作者宕机后将任务重新分配。工作者就是干活的。
- 出现问题。领导者GG了，整个集群都GG。领导者能力不够，集群性能就低。
- 解决方案。多个领导者备份。选举机制。

### 1.1.2 去中心化
- **众生平等**。没有角色分别。
- 自由选择中心。集群成员自发选举领导者。
- 出现问题。脑裂，一个集群因为网络故障，被分成多个彼此无法通信的单独集群，各自为战。
- 解决方案。较小的集群自杀或拒绝服务。

### 1.1.3 集群和分布式
集群偏向物理状态，让多个服务器跑同一个服务。

分布式偏向逻辑状态，把一个服务拆分多个子服务，并部署在不同的服务器上。

粗暴理解：单机->一个全栈；集群->多个全栈；分布式->前后端分离。


## 1.2 CAP定理
对于一个分布式计算系统来说，不能同时满足以下三点。
- 一致性Consistence。系统在操作数据后能从一个一致性状态转移到另一个一致性状态。
- 可用性Availability。系统提供的服务一直处于可用状态，对用户的请求都能在有限时间返回。
- 分区容忍性Partition tolerance。系统在遇到网络分区故障时，仍能对外提供一致性和可用性服务。

所以分区容忍性必不可少，CAP实际上是在一致性和可用性间权衡。
- 一致性 + 分区容忍性。不能访问未同步完成的节点，失去部分可用性。
- 可用性 + 分区容忍性。允许访问全部节点，但数据可能不一致。


## 1.3 BASE理论
BASE是对CAP中一致性和可用性权衡的结果，用来保证 `即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当方式使系统达到最终一致性。`


- 基本可用Basically Available。系统出现故障时，保证核心可用，允许损失部分可用性。
- 软状态Soft State。允许系统数据存在中间状态，即允许系统各个节点间的数据同步存在时延。
- 最终一致性Eventually Consistent。系统所有的数据副本，在一段时间的同步后，最终能达到一致的状态。
 


# 2. 事务
事务要满足ACID的特性，分布式事务指事务的操作在不同的节点上。

## 2.1 2PC
两阶段提交。引入协调者来协调参与者的行为，并决定参与者执行的事务。

### 2.1.1 过程
- 准备阶段。协调者询问参与者事务是否执行成功，参与者返回执行结果。
- 提交阶段。若都成功，协调者通知参与者提交事务，否则协调者通知回滚。

理解为 中央集权。

### 2.1.2 问题
1. 同步阻塞。事务参与者在等待其他事务参与者响应时处于同步阻塞状态，无法做其他事。
2. 单点故障。协调者GG，特别是提交阶段GG，所有参与者都一直阻塞。
3. 数据不一致。提交阶段产生网络波动，部分参与者没有收到消息，导致只有部分提交，数据不一致。
4. 保守。一个节点失败，整个流程回滚。不适用高并发场景。 


## 2.2 3PC
三阶段提交。通过超时机制解决阻塞问题，增加询问阶段。

### 2.2.1 过程
- 询问阶段。协调者询问参与者是否能完成事务，只用回答是或不是。
- 准备阶段。和2pc类似。区别是若有参与者回答 no 或 超时，中断事务。
- 提交阶段。和2pc类似。若参与者等待超时，则默认成功，继续事务提交。


### 2.2.2 问题
一旦超时，系统就可能发生数据不一致的情况。

比如协调者发送的中止命令没有及时被接收，参与者在等待超时后执行了提交事务，导致和回滚事务的参与者间存在数据不一致。




## 2.3 本地消息表
本地消息表和业务数据表放在同一数据库，保证本地事务，用MQ保证最终一致性。


### 2.3.1 过程
- 消息生成方完成写数据操作后向本地消息表发送一个消息，本地事务保证这个消息一定会被写入本地消息表中。
- 将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
- 消息消费方从消息队列中读取一个消息，并执行消息中的操作。


### 2.3.2 问题
消息表耦合到业务系统中，复杂性增加。

# 3. 一致性算法
## 3.1 Paxos
对于多个节点产生的值，能保证选出唯一值。

### 3.1.1 组成
- 提议者Proposer：提议一个值；
- 接受者Acceptor：对每个提议进行投票；
- 告知者Learner：被告知投票的结果，不进行投票。


### 3.1.2 过程
提议格式为 [n,v]，n为唯一序号，v为提议值。

1. Prepare阶段。
    - Proposer向所有Acceptor发送Prepare请求；
    - Acceptor第一次收到请求，则发送Prepare响应，设置当前接受的提议为 [n1,v1]，并保证之后不会接受序号小于 n1 的提议。
    - 后续收到 [n2,v2]，若 n2<n1，直接丢弃；否则，发送Prepare响应，设置当前接受提议为 [n2,v2]，并保证之后不会接受序号小于 n2 的提议。
2. Accept阶段。
    - 一个Proposer接收到超过一半的Acceptor的响应时，发送Accept请求。
3. Learn阶段。
    - Acceptor收到Accept请求时，若序号 > 承诺的最小序号，就发送Learn提议给所有的Learner。
    - Learner发现大多数的Acceptor接受了某个提议，则该提议值就是选出的唯一值。

### 3.1.3 特点
- 正确性。最后只有一个提议值生效。因为接受者只能接受一个提议，且每个提议需要大多数接受者接受。
- 可终止性。最后总有一个提议生效。


## 3.2 Raft
用来竞选主节点。

有三种节点：Follower，Candidate和Leader。


### 3.2.1 单个候选者

- Leader周期性发送心跳包给所有Follower；
- 每个Follower设置随机的超时时间，超过这个时间没有收到心跳包，则变成Candidate，进入竞选。
- Candidate发送投票请求给其它所有节点。
- 其它节点对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。
- Leader 周期性地发送心跳包给 Follower，Follower 接收到心跳包，重新开始计时。

### 3.2.2 多个候选者

- 如果有多个 Follower 成为 Candidate，并获得票数相同，就重新投票。
- 因为每个节点的竞选超时时间随机，所以下一次再次出现多个 Candidate 并获得同样票数的概率很低。

### 3.2.3 数据同步

- 客户端的修改被传入 Leader，并写入修改日志。
- Leader 把修改复制给所有 Follower。
- Leader 等待大多数的 Follower 也进行了修改，然后提交修改。
- Leader 通知所有 Follower 也提交修改，此时所有节点的值达成一致。





